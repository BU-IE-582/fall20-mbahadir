{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to explain what have been obtained, this report will be used. First of all, general information related to dataset can be listed as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pisa Reading Score\n",
    "* Target Variable is Reading Score.\n",
    "* Both Train and Test dataset have more than 200 input.\n",
    "* Dataset has more than 20 Features.\n",
    "* A brief description added to the notebooks at the beggining. \n",
    "\n",
    "## Blog Comment\n",
    "* Target Variable is Reading Score.\n",
    "* Both Train and Test dataset have more than 200 input.\n",
    "* Dataset has more than 20 Features.\n",
    "* A brief description added to the notebooks at the beggining. \n",
    "\n",
    "## Student Evaluation\n",
    "* Target Variable is Reading Score.\n",
    "* Both Train and Test dataset have more than 200 input.\n",
    "* Dataset has more than 20 Features.\n",
    "* A brief description added to the notebooks at the beggining. \n",
    "\n",
    "## Spam Mail\n",
    "* Target Variable is Reading Score.\n",
    "* Both Train and Test dataset have more than 200 input.\n",
    "* Dataset has more than 20 Features.\n",
    "* A brief description added to the notebooks at the beggining. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific Requirements\n",
    "In addition to this requirements, **specific information** related to dataset can be listed as:\n",
    "\n",
    "* Pisa Reading Score and Blog Comment Dataset have regression Problem.\n",
    "* Student Evaluation Test is multi-classification problem.\n",
    "* Spam Mail has imbalance problem in the classification problem.\n",
    "* Blog Comment and Spam Mail datasets have more than 50 features.\n",
    "* Student Evaluation, Reading Score Datasets have categorical independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Achieved Performance Measures for Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, what obtained in the datasets are added to this report. After these performance results, general comments will be made related to models and datasets. Namely, specific information related to models and dataset relationship can be found in the comment in the performance measure parts. At the end, a brief comparision and models' performance in different type of problems will be evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pisa Reading Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different Penalty types are tried in the glmnet package by using mean absolute percentage error and mean square error objective. In these 2 penalty, 3 different lambdas are tried to obtain a model. In addition, all the possible lambda values are checked by plotting the cv.glmnet() results. Predictions are made based on these models. Best model performance are observed in this test dataset. After all, caret() packages are used to make a model can be comparable with other method's models. Namely, caret library is used in all the methods to obtain similar report types. Penalized Regression Approach is made with 3 different lambda values. These are lambda.min, lambda.1se, and random a lambda value achieved in the previous cv.glmnet() process. Model results are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "glmnet \n",
       "\n",
       "2414 samples\n",
       "  23 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 1 times) \n",
       "Summary of sample sizes: 2172, 2170, 2173, 2173, 2172, 2173, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  lambda      RMSE      Rsquared   MAE     \n",
       "   0.2935122  74.18294  0.3113987  59.11383\n",
       "   4.7835176  75.29688  0.3020824  60.35636\n",
       "  13.3104220  80.12992  0.2660190  64.68339\n",
       "\n",
       "Tuning parameter 'alpha' was held constant at a value of 1\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final values used for the model were alpha = 1 and lambda = 0.2935122."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model for Penalized Regression Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE will be the objective parameter for evaluation of the model. In this perpective, **lambda 0.2935122(lambda.min) and MAE Penalty type** is the best tuned parameter for this model for Pisa Reading Score Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The Smallest RMSE value in Penalized Regression Approach: 74.1829393463686\"\n"
     ]
    }
   ],
   "source": [
    "print(paste(\"The Smallest RMSE value in Penalized Regression Approach:\",lm_model$results$RMSE[which.min(lm_model$results$RMSE)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6 different minbucket** values are tried to get best model for Decision Tree. However, in caret library there is no available packages for rpart for tuning the minimal number of observation per tree leaf. So for loop is tried to get information related to effects in the change of number of variable in the tree leaf. In addition, all models tested with **10 different cp value**. After looking the created model's result, the best model is selected. The best RMSE value is selected with considering the minbucket values and cp values. Selected model evaluated 1 more time and named as tr_last. Model results are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CART \n",
       "\n",
       "2414 samples\n",
       "  29 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 2174, 2171, 2173, 2172, 2172, 2172, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  cp    RMSE      Rsquared   MAE     \n",
       "  0.00  80.54359  0.2271929  64.29627\n",
       "  0.01  79.54121  0.2094880  63.67533\n",
       "  0.02  81.09784  0.1780498  65.22498\n",
       "  0.03  81.86471  0.1618356  65.90086\n",
       "  0.04  81.86471  0.1618356  65.90086\n",
       "  0.05  81.86471  0.1618356  65.90086\n",
       "  0.06  83.80165  0.1216110  67.70118\n",
       "  0.07  83.82850  0.1210650  67.73239\n",
       "  0.08  83.82850  0.1210650  67.73239\n",
       "  0.09  83.82850  0.1210650  67.73239\n",
       "  0.10  83.82850  0.1210650  67.73239\n",
       "\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final value used for the model was cp = 0.01."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tr_last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model for Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE will be the objective parameter for evaluation of the model. In this perpective, **cp=0.01 and minbucket = 9** is the best tuned parameter for this model for Pisa Reading Score Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "79.5412116765522"
      ],
      "text/latex": [
       "79.5412116765522"
      ],
      "text/markdown": [
       "79.5412116765522"
      ],
      "text/plain": [
       "[1] 79.54121"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(paste(\"The Smallest RMSE value in Decision Tree:\",tr_last$results$RMSE[which.min(tr_last$results$RMSE)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the homework description, only mtry namely number of selected feature for model is modified. 4 different number is determined. Best model is achived the model with the biggest number of feature. Hovewer, Mean Absolute Error is the smallest in the model with the smallest number of feature. Model results are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "2414 samples\n",
       "  23 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 1 times) \n",
       "Summary of sample sizes: 2173, 2172, 2172, 2173, 2173, 2173, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  RMSE      Rsquared   MAE     \n",
       "   4    74.22778  0.3127169  59.68398\n",
       "   8    74.86137  0.2988596  60.03758\n",
       "  10    75.34474  0.2908063  60.46137\n",
       "  15    75.50474  0.2895483  60.54514\n",
       "\n",
       "Tuning parameter 'splitrule' was held constant at a value of variance\n",
       "\n",
       "Tuning parameter 'min.node.size' was held constant at a value of 5\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final values used for the model were mtry = 4, splitrule = variance\n",
       " and min.node.size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model for Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE will be the objective parameter for evaluation of the model. In this perpective, **mtry=4** is the best tuned parameter for this model for Pisa Reading Score Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The Smallest RMSE value in Random Forest: 74.2277812566288\"\n"
     ]
    }
   ],
   "source": [
    "print(paste(\"The Smallest RMSE value in Random Forest:\",rf_fit$results$RMSE[which.min(rf_fit$results$RMSE)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different 5 shrinkage value, 3 interaction.depth, 5 n.trees values** are tried to obtain the best model. This model is the one which takes the most time to create the models. The reason behind this fact caused from number of tuned parameter in the approach. However, it has the best RMSE value among the models. Model results are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stochastic Gradient Boosting \n",
       "\n",
       "2414 samples\n",
       "  23 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 1 times) \n",
       "Summary of sample sizes: 2173, 2172, 2172, 2171, 2173, 2174, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  shrinkage  interaction.depth  n.trees  RMSE      Rsquared   MAE     \n",
       "  0.1        1                   50      75.47460  0.3126352  60.62038\n",
       "  0.1        1                  100      73.47254  0.3300000  58.69375\n",
       "  0.1        1                  150      72.95822  0.3358354  58.19940\n",
       "  0.1        1                  200      72.80733  0.3378907  58.00691\n",
       "  0.1        1                  250      72.63771  0.3407722  57.90860\n",
       "  0.1        3                   50      73.16656  0.3337547  58.63596\n",
       "  0.1        3                  100      72.34571  0.3459466  57.79544\n",
       "  0.1        3                  150      72.28070  0.3472914  57.76970\n",
       "  0.1        3                  200      72.12923  0.3504537  57.65767\n",
       "  0.1        3                  250      72.17670  0.3501236  57.65177\n",
       "  0.1        5                   50      72.62962  0.3418939  58.13086\n",
       "  0.1        5                  100      72.12213  0.3501401  57.66027\n",
       "  0.1        5                  150      72.23960  0.3489102  57.67923\n",
       "  0.1        5                  200      72.40157  0.3467039  57.85780\n",
       "  0.1        5                  250      72.57940  0.3439994  58.02686\n",
       "  0.3        1                   50      73.10363  0.3342225  58.22410\n",
       "  0.3        1                  100      72.80414  0.3384781  57.98277\n",
       "  0.3        1                  150      72.95011  0.3357024  58.15391\n",
       "  0.3        1                  200      72.53172  0.3435654  57.77987\n",
       "  0.3        1                  250      72.31134  0.3471333  57.71882\n",
       "  0.3        3                   50      72.69742  0.3405160  57.79556\n",
       "  0.3        3                  100      72.96487  0.3376297  58.10455\n",
       "  0.3        3                  150      73.54539  0.3307890  58.65520\n",
       "  0.3        3                  200      74.04792  0.3245085  59.03830\n",
       "  0.3        3                  250      74.44774  0.3203904  59.34712\n",
       "  0.3        5                   50      73.14272  0.3348002  58.48057\n",
       "  0.3        5                  100      73.64867  0.3315839  59.09907\n",
       "  0.3        5                  150      74.25703  0.3243931  59.55848\n",
       "  0.3        5                  200      75.26677  0.3127645  60.12264\n",
       "  0.3        5                  250      76.56458  0.2973616  61.06134\n",
       "  0.5        1                   50      73.12045  0.3333609  58.19274\n",
       "  0.5        1                  100      72.45678  0.3450455  57.75706\n",
       "  0.5        1                  150      72.52661  0.3440706  57.91504\n",
       "  0.5        1                  200      72.60811  0.3434056  57.72905\n",
       "  0.5        1                  250      72.21692  0.3493538  57.52855\n",
       "  0.5        3                   50      73.29377  0.3332815  58.68340\n",
       "  0.5        3                  100      74.11072  0.3253293  59.37377\n",
       "  0.5        3                  150      74.86961  0.3175046  59.98942\n",
       "  0.5        3                  200      75.35855  0.3130353  60.47737\n",
       "  0.5        3                  250      76.00930  0.3062170  60.90497\n",
       "  0.5        5                   50      75.05514  0.3136308  60.25197\n",
       "  0.5        5                  100      77.85234  0.2830813  62.12729\n",
       "  0.5        5                  150      79.66701  0.2644785  63.74646\n",
       "  0.5        5                  200      81.89056  0.2397880  65.42251\n",
       "  0.5        5                  250      83.40476  0.2288027  67.05508\n",
       "\n",
       "Tuning parameter 'n.minobsinnode' was held constant at a value of 20\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final values used for the model were n.trees = 100, interaction.depth =\n",
       " 5, shrinkage = 0.1 and n.minobsinnode = 20."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gbm_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model for Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE will be the objective parameter for evaluation of the model. In this perpective, **n.trees = 100 interaction.depth =\n",
    " 5, shrinkage = 0.1 and n.minobsinnode = 20** is the best tuned parameter for this model for Pisa Reading Score Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The Smallest RMSE value in Stochastic Gradient Boosting: 72.1221273300718\"\n"
     ]
    }
   ],
   "source": [
    "print(paste(\"The Smallest RMSE value in Stochastic Gradient Boosting:\",gbm_fit$results$RMSE[which.min(gbm_fit$results$RMSE)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparision of Results in Training and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=list(\"Linear Regression with Lasso Results\"=lm_model$results$RMSE[which.min(lm_model$results$RMSE)],\n",
    "            \"Decision Tree Results\"=    tr_last$results$RMSE[which.min(tr_last$results$RMSE)],\n",
    "            \"Random Forest Results\"=    rf_fit$results$RMSE[which.min(rf_fit$results$RMSE)],\n",
    "            \"Result Stochastic Gradient Boosting Results\"=    gbm_fit$results$RMSE[which.min(gbm_fit$results$RMSE)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE in The Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Linear Regression with Lasso Results</th><th scope=col>Decision Tree Results</th><th scope=col>Random Forest Results</th><th scope=col>Result Stochastic Gradient Boosting Results</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>74.18294</td><td>79.54121</td><td>74.22778</td><td>72.12213</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{llll}\n",
       " Linear Regression with Lasso Results & Decision Tree Results & Random Forest Results & Result Stochastic Gradient Boosting Results\\\\\n",
       "\\hline\n",
       "\t 74.18294 & 79.54121 & 74.22778 & 72.12213\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Linear Regression with Lasso Results | Decision Tree Results | Random Forest Results | Result Stochastic Gradient Boosting Results |\n",
       "|---|---|---|---|\n",
       "| 74.18294 | 79.54121 | 74.22778 | 72.12213 |\n",
       "\n"
      ],
      "text/plain": [
       "     Linear Regression with Lasso Results Decision Tree Results\n",
       "[1,] 74.18294                             79.54121             \n",
       "     Random Forest Results Result Stochastic Gradient Boosting Results\n",
       "[1,] 74.22778              72.12213                                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>name</th><th scope=col>n</th><th scope=col>mean</th><th scope=col>sd</th><th scope=col>FBias</th><th scope=col>MAPE</th><th scope=col>RMSE</th><th scope=col>MAD</th><th scope=col>WMAPE</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Pisa Data Set for Lasso Function with min lambda and mse objective</td><td>990                                                               </td><td>519.8643                                                          </td><td>88.80043                                                          </td><td>0.007031004                                                       </td><td>0.1220864                                                         </td><td>2.435895                                                          </td><td>60.65549                                                          </td><td>0.1166756                                                         </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllll}\n",
       " name & n & mean & sd & FBias & MAPE & RMSE & MAD & WMAPE\\\\\n",
       "\\hline\n",
       "\t Pisa Data Set for Lasso Function with min lambda and mse objective & 990                                                                & 519.8643                                                           & 88.80043                                                           & 0.007031004                                                        & 0.1220864                                                          & 2.435895                                                           & 60.65549                                                           & 0.1166756                                                         \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| name | n | mean | sd | FBias | MAPE | RMSE | MAD | WMAPE |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| Pisa Data Set for Lasso Function with min lambda and mse objective | 990                                                                | 519.8643                                                           | 88.80043                                                           | 0.007031004                                                        | 0.1220864                                                          | 2.435895                                                           | 60.65549                                                           | 0.1166756                                                          |\n",
       "\n"
      ],
      "text/plain": [
       "  name                                                               n  \n",
       "1 Pisa Data Set for Lasso Function with min lambda and mse objective 990\n",
       "  mean     sd       FBias       MAPE      RMSE     MAD      WMAPE    \n",
       "1 519.8643 88.80043 0.007031004 0.1220864 2.435895 60.65549 0.1166756"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>name</th><th scope=col>n</th><th scope=col>mean</th><th scope=col>sd</th><th scope=col>FBias</th><th scope=col>MAPE</th><th scope=col>RMSE</th><th scope=col>MAD</th><th scope=col>WMAPE</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Decision Tree with CV for Pisa Dataset</td><td>990                                   </td><td>519.8643                              </td><td>88.80043                              </td><td>0.007325851                           </td><td>0.1262667                             </td><td>2.506513                              </td><td>62.69629                              </td><td>0.1206013                             </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllll}\n",
       " name & n & mean & sd & FBias & MAPE & RMSE & MAD & WMAPE\\\\\n",
       "\\hline\n",
       "\t Decision Tree with CV for Pisa Dataset & 990                                    & 519.8643                               & 88.80043                               & 0.007325851                            & 0.1262667                              & 2.506513                               & 62.69629                               & 0.1206013                             \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| name | n | mean | sd | FBias | MAPE | RMSE | MAD | WMAPE |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| Decision Tree with CV for Pisa Dataset | 990                                    | 519.8643                               | 88.80043                               | 0.007325851                            | 0.1262667                              | 2.506513                               | 62.69629                               | 0.1206013                              |\n",
       "\n"
      ],
      "text/plain": [
       "  name                                   n   mean     sd       FBias      \n",
       "1 Decision Tree with CV for Pisa Dataset 990 519.8643 88.80043 0.007325851\n",
       "  MAPE      RMSE     MAD      WMAPE    \n",
       "1 0.1262667 2.506513 62.69629 0.1206013"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>name</th><th scope=col>n</th><th scope=col>mean</th><th scope=col>sd</th><th scope=col>FBias</th><th scope=col>MAPE</th><th scope=col>RMSE</th><th scope=col>MAD</th><th scope=col>WMAPE</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>First Data Set for Random Forest</td><td>990                             </td><td>517.8192                        </td><td>45.48294                        </td><td>-0.003949342                    </td><td>0.1157326                       </td><td>2.383545                        </td><td>59.41911                        </td><td>0.1147487                       </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllll}\n",
       " name & n & mean & sd & FBias & MAPE & RMSE & MAD & WMAPE\\\\\n",
       "\\hline\n",
       "\t First Data Set for Random Forest & 990                              & 517.8192                         & 45.48294                         & -0.003949342                     & 0.1157326                        & 2.383545                         & 59.41911                         & 0.1147487                       \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| name | n | mean | sd | FBias | MAPE | RMSE | MAD | WMAPE |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| First Data Set for Random Forest | 990                              | 517.8192                         | 45.48294                         | -0.003949342                     | 0.1157326                        | 2.383545                         | 59.41911                         | 0.1147487                        |\n",
       "\n"
      ],
      "text/plain": [
       "  name                             n   mean     sd       FBias        MAPE     \n",
       "1 First Data Set for Random Forest 990 517.8192 45.48294 -0.003949342 0.1157326\n",
       "  RMSE     MAD      WMAPE    \n",
       "1 2.383545 59.41911 0.1147487"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>name</th><th scope=col>n</th><th scope=col>mean</th><th scope=col>sd</th><th scope=col>FBias</th><th scope=col>MAPE</th><th scope=col>RMSE</th><th scope=col>MAD</th><th scope=col>WMAPE</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>First Data Set for Stochastic Gradient Boosting</td><td>990                                            </td><td>516.5296                                       </td><td>52.44005                                       </td><td>-0.00645595                                    </td><td>0.1154572                                      </td><td>2.364434                                       </td><td>58.9861                                        </td><td>0.1141969                                      </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllll}\n",
       " name & n & mean & sd & FBias & MAPE & RMSE & MAD & WMAPE\\\\\n",
       "\\hline\n",
       "\t First Data Set for Stochastic Gradient Boosting & 990                                             & 516.5296                                        & 52.44005                                        & -0.00645595                                     & 0.1154572                                       & 2.364434                                        & 58.9861                                         & 0.1141969                                      \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| name | n | mean | sd | FBias | MAPE | RMSE | MAD | WMAPE |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| First Data Set for Stochastic Gradient Boosting | 990                                             | 516.5296                                        | 52.44005                                        | -0.00645595                                     | 0.1154572                                       | 2.364434                                        | 58.9861                                         | 0.1141969                                       |\n",
       "\n"
      ],
      "text/plain": [
       "  name                                            n   mean     sd      \n",
       "1 First Data Set for Stochastic Gradient Boosting 990 516.5296 52.44005\n",
       "  FBias       MAPE      RMSE     MAD     WMAPE    \n",
       "1 -0.00645595 0.1154572 2.364434 58.9861 0.1141969"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perf_dt(\"Pisa Data Set for Lasso Function with min lambda and mse objective\", as.numeric(pisa_test$readingScore), prediction_pra_mse_pisa_min)\n",
    "perf_dt(\"Decision Tree with CV for Pisa Dataset\",pisa_test_dt$readingScore,as.numeric(predicted_pisa_dt))\n",
    "perf_dt(\"First Data Set for Random Forest\", as.numeric(RandomForest_pisa), as.numeric(pisa_test$readingScore))\n",
    "perf_dt(\"First Data Set for Stochastic Gradient Boosting\", as.numeric(predicted_pisa_sgb), as.numeric(pisa_test$readingScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models got similar performance as achieved in the training dataset. Namely, it is hard to say that there is an over-fitting or under-fitting. Reading Score Estimation can be described as regression problem, so having the worst performance measure both training and test data sets aren't a suprise for decision tree model. In addition to under/over-fitting, it can be said that error rates are consistent for both datasets, which can be observed looking the correlation between the results pf performance of the models in both dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blog Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different Penalty types are tried in the glmnet package by using mean absolute percentage error and mean square error objective. In these 2 penalty, 3 different lambdas are tried to obtain a model. In addition, all the possible lambda values are checked by plotting the cv.glmnet() results. Predictions are made based on these models. Best model performance are observed in this test dataset. After all, caret() packages are used to make a model can be comparable with other method's models. Namely, caret library is used in all the methods to obtain similar report types. Penalized Regression Approach is made with 3 different lambda values. These are lambda.min, lambda.1se, and random a lambda value achieved in the previous cv.glmnet() process. Model results are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "glmnet \n",
       "\n",
       "8023 samples\n",
       " 280 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 1 times) \n",
       "Summary of sample sizes: 7221, 7220, 7220, 7220, 7222, 7220, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  lambda    RMSE      Rsquared   MAE      \n",
       "  2.441102  24.42847  0.2050926   9.869664\n",
       "  3.226993  24.61889  0.2009448  10.022668\n",
       "  4.681814  25.07599  0.1863053  10.510031\n",
       "\n",
       "Tuning parameter 'alpha' was held constant at a value of 1\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final values used for the model were alpha = 1 and lambda = 2.441102."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model for Penalized Regression Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE will be the objective parameter for evaluation of the model. In this perpective, **lambda 2.441102(lambda.min) and MAE Penalty type** is the best tuned parameter for this model for Blog Comment Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The Smallest RMSE value in Penalized Regression Approach: 24.4284740530075\"\n"
     ]
    }
   ],
   "source": [
    "print(paste(\"The Smallest RMSE value in Penalized Regression Approach:\",lm_model$results$RMSE[which.min(lm_model$results$RMSE)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6 different minbucket** values are tried to get best model for Decision Tree. However, in caret library there is no available packages for rpart for tuning the minimal number of observation per tree leaf. So for loop is tried to get information related to effects in the change of number of variable in the tree leaf. In addition, all models tested with **10 different cp value**. After looking the created model's result, the best model is selected. The best RMSE value is selected with considering the minbucket values and cp values. Selected model evaluated 1 more time and named as tr_last. Model results are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CART \n",
       "\n",
       "8023 samples\n",
       " 280 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 1 times) \n",
       "Summary of sample sizes: 7220, 7220, 7221, 7222, 7221, 7220, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  cp    RMSE      Rsquared   MAE      \n",
       "  0.00  22.57559  0.3897072   7.384471\n",
       "  0.01  23.31100  0.2881720   8.693458\n",
       "  0.02  23.59941  0.2619470   9.289978\n",
       "  0.03  23.57051  0.2572654   9.434858\n",
       "  0.04  24.22841  0.2119279   9.718694\n",
       "  0.05  24.51101  0.1872444   9.984797\n",
       "  0.06  24.51033  0.1872444   9.990837\n",
       "  0.07  25.27038  0.1352021  10.349000\n",
       "  0.08  25.27885  0.1329492  10.415419\n",
       "  0.09  25.27885  0.1329492  10.415419\n",
       "  0.10  25.27885  0.1329492  10.415419\n",
       "\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final value used for the model was cp = 0."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tr_last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model for Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE will be the objective parameter for evaluation of the model. In this perpective, **cp=0 and minbucket = 9** is the best tuned parameter for this model for Blog Comment Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The Smallest RMSE value in Decision Tree: 22.5755890810299\"\n"
     ]
    }
   ],
   "source": [
    "print(paste(\"The Smallest RMSE value in Decision Tree:\",tr_last$results$RMSE[which.min(tr_last$results$RMSE)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the homework description, only mtry namely number of selected feature for model is modified. 4 different number is determined. Best model is achived the model with the biggest number of feature. Hovewer, Mean Absolute Error is the smallest in the model with the smallest number of feature. Model results are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "8023 samples\n",
       " 280 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 1 times) \n",
       "Summary of sample sizes: 7220, 7221, 7220, 7221, 7221, 7221, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  RMSE      Rsquared   MAE     \n",
       "   4    23.12233  0.3410507  9.550554\n",
       "   8    20.40609  0.4818724  7.921203\n",
       "  10    19.78936  0.5023333  7.523441\n",
       "  15    19.20015  0.5178478  6.988483\n",
       "\n",
       "Tuning parameter 'splitrule' was held constant at a value of variance\n",
       "\n",
       "Tuning parameter 'min.node.size' was held constant at a value of 5\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final values used for the model were mtry = 15, splitrule = variance\n",
       " and min.node.size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model for Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE will be the objective parameter for evaluation of the model. In this perpective, **mtry=15** is the best tuned parameter for this model for Blog Comment Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The Smallest RMSE value in Random Forest: 19.2001454757668\"\n"
     ]
    }
   ],
   "source": [
    "print(paste(\"The Smallest RMSE value in Random Forest:\",rf_fit$results$RMSE[which.min(rf_fit$results$RMSE)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different 5 shrinkage value, 3 interaction.depth, 5 n.trees values** are tried to obtain the best model. This model is the one which takes the most time to create the models. The reason behind this fact caused from number of tuned parameter in the approach. However, it has the best RMSE value among the models. Model results are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stochastic Gradient Boosting \n",
       "\n",
       "8023 samples\n",
       " 270 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 1 times) \n",
       "Summary of sample sizes: 7222, 7220, 7221, 7220, 7220, 7221, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  shrinkage  interaction.depth  n.trees  RMSE      Rsquared   MAE     \n",
       "  0.1        1                   25      23.51668  0.2849392  9.506556\n",
       "  0.1        1                   50      22.80351  0.3032841  8.832547\n",
       "  0.1        1                   75      22.65047  0.3098092  8.775545\n",
       "  0.1        1                  100      22.63235  0.3104455  8.889400\n",
       "  0.1        1                  125      22.61024  0.3117079  8.963804\n",
       "  0.1        3                   25      21.73289  0.3743305  8.388996\n",
       "  0.1        3                   50      21.29655  0.3899361  8.003750\n",
       "  0.1        3                   75      21.22808  0.3937943  7.952887\n",
       "  0.1        3                  100      21.24836  0.3933181  7.957694\n",
       "  0.1        3                  125      21.13266  0.3985761  7.919171\n",
       "  0.1        5                   25      21.26734  0.3942678  8.123113\n",
       "  0.1        5                   50      20.84592  0.4144188  7.840904\n",
       "  0.1        5                   75      20.78450  0.4185088  7.786754\n",
       "  0.1        5                  100      20.73843  0.4216444  7.728035\n",
       "  0.1        5                  125      20.73801  0.4216841  7.708666\n",
       "  0.3        1                   25      22.75991  0.3033429  8.904484\n",
       "  0.3        1                   50      22.65425  0.3091259  9.147864\n",
       "  0.3        1                   75      22.68049  0.3069109  9.257940\n",
       "  0.3        1                  100      22.77801  0.3042892  9.366215\n",
       "  0.3        1                  125      22.77007  0.3031796  9.357287\n",
       "  0.3        3                   25      21.28949  0.3898692  8.032260\n",
       "  0.3        3                   50      21.25482  0.3920144  7.976828\n",
       "  0.3        3                   75      21.22668  0.3973066  8.035365\n",
       "  0.3        3                  100      21.21552  0.3991163  8.059988\n",
       "  0.3        3                  125      21.10101  0.4055901  8.013156\n",
       "  0.3        5                   25      21.10461  0.4021767  7.928543\n",
       "  0.3        5                   50      21.16008  0.4007089  8.003955\n",
       "  0.3        5                   75      21.25418  0.3979742  8.047919\n",
       "  0.3        5                  100      21.19502  0.4028374  8.094952\n",
       "  0.3        5                  125      21.09334  0.4105282  8.136736\n",
       "  0.5        1                   25      22.76220  0.3027179  9.365923\n",
       "  0.5        1                   50      22.73948  0.3035954  9.498467\n",
       "  0.5        1                   75      22.74929  0.3061885  9.567049\n",
       "  0.5        1                  100      22.91499  0.2977383  9.577378\n",
       "  0.5        1                  125      22.78342  0.3039703  9.621800\n",
       "  0.5        3                   25      21.71309  0.3709643  8.255165\n",
       "  0.5        3                   50      21.79729  0.3697018  8.442765\n",
       "  0.5        3                   75      21.73295  0.3773797  8.484411\n",
       "  0.5        3                  100      21.77754  0.3752535  8.594138\n",
       "  0.5        3                  125      21.88520  0.3726704  8.727003\n",
       "  0.5        5                   25      21.70753  0.3759009  8.311945\n",
       "  0.5        5                   50      21.75311  0.3799195  8.508803\n",
       "  0.5        5                   75      21.67053  0.3851793  8.716010\n",
       "  0.5        5                  100      21.83928  0.3843722  8.839735\n",
       "  0.5        5                  125      21.88843  0.3866019  8.871959\n",
       "\n",
       "Tuning parameter 'n.minobsinnode' was held constant at a value of 20\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final values used for the model were n.trees = 125, interaction.depth =\n",
       " 5, shrinkage = 0.1 and n.minobsinnode = 20."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gbm_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model for Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE will be the objective parameter for evaluation of the model. In this perpective, **n.trees = 125 interaction.depth =\n",
    " 5, shrinkage = 0.1 and n.minobsinnode = 20** is the best tuned parameter for this model for Number of Blog Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The Smallest RMSE value in Stochastic Gradient Boosting: 20.7380054790217\"\n"
     ]
    }
   ],
   "source": [
    "print(paste(\"The Smallest RMSE value in Stochastic Gradient Boosting:\",gbm_fit$results$RMSE[which.min(gbm_fit$results$RMSE)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparision of Results in Training and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=list(\"Linear Regression with Lasso Results\"=lm_model$results$RMSE[which.min(lm_model$results$RMSE)],\n",
    "            \"Decision Tree Results\"=    tr_last$results$RMSE[which.min(tr_last$results$RMSE)],\n",
    "            \"Random Forest Results\"=    rf_fit$results$RMSE[which.min(rf_fit$results$RMSE)],\n",
    "            \"Result Stochastic Gradient Boosting Results\"=    gbm_fit$results$RMSE[which.min(gbm_fit$results$RMSE)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE in The Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Linear Regression with Lasso Results</th><th scope=col>Decision Tree Results</th><th scope=col>Random Forest Results</th><th scope=col>Result Stochastic Gradient Boosting Results</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>24.42847</td><td>22.57559</td><td>19.20015</td><td>20.73801</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{llll}\n",
       " Linear Regression with Lasso Results & Decision Tree Results & Random Forest Results & Result Stochastic Gradient Boosting Results\\\\\n",
       "\\hline\n",
       "\t 24.42847 & 22.57559 & 19.20015 & 20.73801\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Linear Regression with Lasso Results | Decision Tree Results | Random Forest Results | Result Stochastic Gradient Boosting Results |\n",
       "|---|---|---|---|\n",
       "| 24.42847 | 22.57559 | 19.20015 | 20.73801 |\n",
       "\n"
      ],
      "text/plain": [
       "     Linear Regression with Lasso Results Decision Tree Results\n",
       "[1,] 24.42847                             22.57559             \n",
       "     Random Forest Results Result Stochastic Gradient Boosting Results\n",
       "[1,] 19.20015              20.73801                                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>name</th><th scope=col>n</th><th scope=col>mean</th><th scope=col>sd</th><th scope=col>FBias</th><th scope=col>MAPE</th><th scope=col>RMSE</th><th scope=col>MAD</th><th scope=col>WMAPE</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Blog Comment Data Set for Lasso Function with min lambda and mse objective</td><td>1977                                                                      </td><td>7.152757                                                                  </td><td>16.66787                                                                  </td><td>-0.2301968                                                                </td><td>Inf                                                                       </td><td>0.3298626                                                                 </td><td>8.726155                                                                  </td><td>1.219971                                                                  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllll}\n",
       " name & n & mean & sd & FBias & MAPE & RMSE & MAD & WMAPE\\\\\n",
       "\\hline\n",
       "\t Blog Comment Data Set for Lasso Function with min lambda and mse objective & 1977                                                                       & 7.152757                                                                   & 16.66787                                                                   & -0.2301968                                                                 & Inf                                                                        & 0.3298626                                                                  & 8.726155                                                                   & 1.219971                                                                  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| name | n | mean | sd | FBias | MAPE | RMSE | MAD | WMAPE |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| Blog Comment Data Set for Lasso Function with min lambda and mse objective | 1977                                                                       | 7.152757                                                                   | 16.66787                                                                   | -0.2301968                                                                 | Inf                                                                        | 0.3298626                                                                  | 8.726155                                                                   | 1.219971                                                                   |\n",
       "\n"
      ],
      "text/plain": [
       "  name                                                                      \n",
       "1 Blog Comment Data Set for Lasso Function with min lambda and mse objective\n",
       "  n    mean     sd       FBias      MAPE RMSE      MAD      WMAPE   \n",
       "1 1977 7.152757 16.66787 -0.2301968 Inf  0.3298626 8.726155 1.219971"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>name</th><th scope=col>n</th><th scope=col>mean</th><th scope=col>sd</th><th scope=col>FBias</th><th scope=col>MAPE</th><th scope=col>RMSE</th><th scope=col>MAD</th><th scope=col>WMAPE</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Decision Tree with CV for Blog Comment Dataset</td><td>1977                                          </td><td>7.152757                                      </td><td>16.66787                                      </td><td>-0.211185                                     </td><td>NaN                                           </td><td>0.3807407                                     </td><td>6.082319                                      </td><td>0.8503462                                     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllll}\n",
       " name & n & mean & sd & FBias & MAPE & RMSE & MAD & WMAPE\\\\\n",
       "\\hline\n",
       "\t Decision Tree with CV for Blog Comment Dataset & 1977                                           & 7.152757                                       & 16.66787                                       & -0.211185                                      & NaN                                            & 0.3807407                                      & 6.082319                                       & 0.8503462                                     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| name | n | mean | sd | FBias | MAPE | RMSE | MAD | WMAPE |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| Decision Tree with CV for Blog Comment Dataset | 1977                                           | 7.152757                                       | 16.66787                                       | -0.211185                                      | NaN                                            | 0.3807407                                      | 6.082319                                       | 0.8503462                                      |\n",
       "\n"
      ],
      "text/plain": [
       "  name                                           n    mean     sd      \n",
       "1 Decision Tree with CV for Blog Comment Dataset 1977 7.152757 16.66787\n",
       "  FBias     MAPE RMSE      MAD      WMAPE    \n",
       "1 -0.211185 NaN  0.3807407 6.082319 0.8503462"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>name</th><th scope=col>n</th><th scope=col>mean</th><th scope=col>sd</th><th scope=col>FBias</th><th scope=col>MAPE</th><th scope=col>RMSE</th><th scope=col>MAD</th><th scope=col>WMAPE</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>First Data Set for Random Forest</td><td>1977                            </td><td>7.152757                        </td><td>16.66787                        </td><td>-0.2323384                      </td><td>Inf                             </td><td>0.2484009                       </td><td>5.514749                        </td><td>0.7709963                       </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllll}\n",
       " name & n & mean & sd & FBias & MAPE & RMSE & MAD & WMAPE\\\\\n",
       "\\hline\n",
       "\t First Data Set for Random Forest & 1977                             & 7.152757                         & 16.66787                         & -0.2323384                       & Inf                              & 0.2484009                        & 5.514749                         & 0.7709963                       \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| name | n | mean | sd | FBias | MAPE | RMSE | MAD | WMAPE |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| First Data Set for Random Forest | 1977                             | 7.152757                         | 16.66787                         | -0.2323384                       | Inf                              | 0.2484009                        | 5.514749                         | 0.7709963                        |\n",
       "\n"
      ],
      "text/plain": [
       "  name                             n    mean     sd       FBias      MAPE\n",
       "1 First Data Set for Random Forest 1977 7.152757 16.66787 -0.2323384 Inf \n",
       "  RMSE      MAD      WMAPE    \n",
       "1 0.2484009 5.514749 0.7709963"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>name</th><th scope=col>n</th><th scope=col>mean</th><th scope=col>sd</th><th scope=col>FBias</th><th scope=col>MAPE</th><th scope=col>RMSE</th><th scope=col>MAD</th><th scope=col>WMAPE</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>First Data Set for Stochastic Gradient Boosting</td><td>1977                                           </td><td>7.152757                                       </td><td>16.66787                                       </td><td>-0.1954518                                     </td><td>Inf                                            </td><td>0.3035566                                      </td><td>6.246727                                       </td><td>0.8733313                                      </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllll}\n",
       " name & n & mean & sd & FBias & MAPE & RMSE & MAD & WMAPE\\\\\n",
       "\\hline\n",
       "\t First Data Set for Stochastic Gradient Boosting & 1977                                            & 7.152757                                        & 16.66787                                        & -0.1954518                                      & Inf                                             & 0.3035566                                       & 6.246727                                        & 0.8733313                                      \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| name | n | mean | sd | FBias | MAPE | RMSE | MAD | WMAPE |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| First Data Set for Stochastic Gradient Boosting | 1977                                            | 7.152757                                        | 16.66787                                        | -0.1954518                                      | Inf                                             | 0.3035566                                       | 6.246727                                        | 0.8733313                                       |\n",
       "\n"
      ],
      "text/plain": [
       "  name                                            n    mean     sd      \n",
       "1 First Data Set for Stochastic Gradient Boosting 1977 7.152757 16.66787\n",
       "  FBias      MAPE RMSE      MAD      WMAPE    \n",
       "1 -0.1954518 Inf  0.3035566 6.246727 0.8733313"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perf_dt(\"Blog Comment Data Set for Lasso Function with min lambda and mse objective\", as.numeric(blog_test$target), prediction_pra_mse_blog_1se)\n",
    "perf_dt(\"Decision Tree with CV for Blog Comment Dataset\",blog_test$target,as.numeric(predicted_blog_dt))\n",
    "perf_dt(\"First Data Set for Random Forest\", as.numeric(blog_test$target), as.numeric(RandomForest_blog))\n",
    "perf_dt(\"First Data Set for Stochastic Gradient Boosting\", as.numeric(blog_test$target), as.numeric(predicted_blog_sgb) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models got similar performance as achieved in the training dataset. Namely, it is hard to say that there is an over-fitting or under-fitting. Estimation of number of blog comment  can be described as regression problem, so having the worst performance measure both training and test data sets aren't a suprise for decision tree model. In addition to under/over-fitting, it can be said that error rates are consistent for both datasets, which can be observed looking the correlation between the results pf performance of the models in both dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction   1   2   3   4   5\n",
       "         1 249  28  61  49  37\n",
       "         2   0   1   0   1   0\n",
       "         3  57  70 255 155  60\n",
       "         4   9   8  23  29  14\n",
       "         5   9   3  16  11  19\n",
       "\n",
       "Overall Statistics\n",
       "                                          \n",
       "               Accuracy : 0.4751          \n",
       "                 95% CI : (0.4461, 0.5042)\n",
       "    No Information Rate : 0.305           \n",
       "    P-Value [Acc > NIR] : < 2.2e-16       \n",
       "                                          \n",
       "                  Kappa : 0.2724          \n",
       "                                          \n",
       " Mcnemar's Test P-Value : < 2.2e-16       \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: 1  Class: 2 Class: 3 Class: 4 Class: 5\n",
       "Precision              0.5873 0.5000000   0.4271  0.34940  0.32759\n",
       "Recall                 0.7685 0.0090909   0.7183  0.11837  0.14615\n",
       "F1                     0.6658 0.0178571   0.5357  0.17683  0.20213\n",
       "Prevalence             0.2784 0.0945017   0.3050  0.21048  0.11168\n",
       "Detection Rate         0.2139 0.0008591   0.2191  0.02491  0.01632\n",
       "Detection Prevalence   0.3643 0.0017182   0.5129  0.07131  0.04983\n",
       "Balanced Accuracy      0.7801 0.5040711   0.6478  0.52980  0.55422"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrp=confusionMatrix(data = as.factor(prediction_pra_mae_min), reference = as.factor(test$y), mode = \"prec_recall\")\n",
    "lrp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction   1   2   3   4   5\n",
       "         1 257  14  40  24  32\n",
       "         2   0   0   0   0   0\n",
       "         3  67  96 315 221  98\n",
       "         4   0   0   0   0   0\n",
       "         5   0   0   0   0   0\n",
       "\n",
       "Overall Statistics\n",
       "                                          \n",
       "               Accuracy : 0.4914          \n",
       "                 95% CI : (0.4623, 0.5205)\n",
       "    No Information Rate : 0.305           \n",
       "    P-Value [Acc > NIR] : < 2.2e-16       \n",
       "                                          \n",
       "                  Kappa : 0.277           \n",
       "                                          \n",
       " Mcnemar's Test P-Value : NA              \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: 1 Class: 2 Class: 3 Class: 4 Class: 5\n",
       "Precision              0.7003       NA   0.3952       NA       NA\n",
       "Recall                 0.7932   0.0000   0.8873   0.0000   0.0000\n",
       "F1                     0.7438       NA   0.5469       NA       NA\n",
       "Prevalence             0.2784   0.0945   0.3050   0.2105   0.1117\n",
       "Detection Rate         0.2208   0.0000   0.2706   0.0000   0.0000\n",
       "Detection Prevalence   0.3153   0.0000   0.6847   0.0000   0.0000\n",
       "Balanced Accuracy      0.8311   0.5000   0.6458   0.5000   0.5000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dtr=confusionMatrix(data = as.factor(predicted_std), reference = as.factor(test$y), mode = \"prec_recall\")\n",
    "dtr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction   1   2   3   4   5\n",
       "         1 242  11  37  20  23\n",
       "         2   0  14   7   6   3\n",
       "         3  58  70 246 137  56\n",
       "         4  15  13  47  65  24\n",
       "         5   9   2  18  17  24\n",
       "\n",
       "Overall Statistics\n",
       "                                          \n",
       "               Accuracy : 0.5077          \n",
       "                 95% CI : (0.4786, 0.5368)\n",
       "    No Information Rate : 0.305           \n",
       "    P-Value [Acc > NIR] : < 2.2e-16       \n",
       "                                          \n",
       "                  Kappa : 0.3284          \n",
       "                                          \n",
       " Mcnemar's Test P-Value : < 2.2e-16       \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: 1 Class: 2 Class: 3 Class: 4 Class: 5\n",
       "Precision              0.7267  0.46667   0.4339  0.39634  0.34286\n",
       "Recall                 0.7469  0.12727   0.6930  0.26531  0.18462\n",
       "F1                     0.7367  0.20000   0.5336  0.31785  0.24000\n",
       "Prevalence             0.2784  0.09450   0.3050  0.21048  0.11168\n",
       "Detection Rate         0.2079  0.01203   0.2113  0.05584  0.02062\n",
       "Detection Prevalence   0.2861  0.02577   0.4871  0.14089  0.06014\n",
       "Balanced Accuracy      0.8193  0.55605   0.6481  0.57879  0.57006"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rfr=confusionMatrix(data = as.factor(RandomForest_std), reference = as.factor(test$y), mode = \"prec_recall\")\n",
    "rfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction   1   2   3   4   5\n",
       "         1 254  11  33  21  30\n",
       "         2   1  13  12   4   2\n",
       "         3  47  70 243 141  48\n",
       "         4  15  12  45  65  26\n",
       "         5   7   4  22  14  24\n",
       "\n",
       "Overall Statistics\n",
       "                                          \n",
       "               Accuracy : 0.5146          \n",
       "                 95% CI : (0.4855, 0.5437)\n",
       "    No Information Rate : 0.305           \n",
       "    P-Value [Acc > NIR] : < 2.2e-16       \n",
       "                                          \n",
       "                  Kappa : 0.3385          \n",
       "                                          \n",
       " Mcnemar's Test P-Value : < 2.2e-16       \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: 1 Class: 2 Class: 3 Class: 4 Class: 5\n",
       "Precision              0.7278  0.40625   0.4426  0.39877  0.33803\n",
       "Recall                 0.7840  0.11818   0.6845  0.26531  0.18462\n",
       "F1                     0.7548  0.18310   0.5376  0.31863  0.23881\n",
       "Prevalence             0.2784  0.09450   0.3050  0.21048  0.11168\n",
       "Detection Rate         0.2182  0.01117   0.2088  0.05584  0.02062\n",
       "Detection Prevalence   0.2998  0.02749   0.4716  0.14003  0.06100\n",
       "Balanced Accuracy      0.8354  0.55008   0.6531  0.57933  0.56958"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sgbr=confusionMatrix(data = as.factor(predicted_sgb), reference = as.factor(test$y), mode = \"prec_recall\")\n",
    "sgbr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking the Accuracy, it can be said that best performance are optained in the Random Forest and Stochastic Gradient Boosting. Indeed, these 2 model's performance is too similar, which can be understood by looking the Precision, Recall, and F1 score of the models. The biggest difference occurs in the precision and recall values of the model. So, main objective can determine which model must be used. Moreover, performance of the other two model is also similar. Indeed decision tree's approach is more proper for classification problems. The reason behind the similarity can be explained by complexity of the models. Namely, Decision Tree's complexity is less than constructed Linear Regression with lasso, but it still has similar performance compared to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different Penalty types are tried in the glmnet package by using mean absolute percentage error and mean square error objective. In these 2 penalty, 3 different lambdas are tried to obtain a model. In addition, all the possible lambda values are checked by plotting the cv.glmnet() results. Predictions are made based on these models. Best model performance are observed in this test dataset. After all, caret() packages are used to make a model can be comparable with other method's models. Namely, caret library is used in all the methods to obtain similar report types. Penalized Regression Approach is made with 3 different lambda values. These are lambda.min, lambda.1se, and random a lambda value achieved in the previous cv.glmnet() process. Model results are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "glmnet \n",
       "\n",
       "4656 samples\n",
       "  32 predictor\n",
       "   5 classes: '1', '2', '3', '4', '5' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 1 times) \n",
       "Summary of sample sizes: 4190, 4190, 4190, 4191, 4190, 4192, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  lambda       Accuracy   Kappa    \n",
       "  0.002179314  0.5199788  0.3445326\n",
       "  0.007304193  0.5158988  0.3296490\n",
       "  0.098829223  0.3047682  0.0000000\n",
       "\n",
       "Tuning parameter 'alpha' was held constant at a value of 1\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were alpha = 1 and lambda = 0.002179314."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model for Penalized Regression Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE will be the objective parameter for evaluation of the model. In this perpective, **alpha = 1, lambda = 0.002179314 and MAE Penalty type** is the best tuned parameter for this model for Student Evaluation Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy \n",
      "0.4750859 \n"
     ]
    }
   ],
   "source": [
    "print(lrp$overall[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6 different minbucket** values are tried to get best model for Decision Tree. However, in caret library there is no available packages for rpart for tuning the minimal number of observation per tree leaf. So for loop is tried to get information related to effects in the change of number of variable in the tree leaf. In addition, all models tested with **10 different cp value**. After looking the created model's result, the best model is selected. The best Accuracy value is selected with considering the minbucket values and cp values. Selected model evaluated 1 more time and named as tr_last. Model results are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CART \n",
       "\n",
       "4656 samples\n",
       " 164 predictor\n",
       "   5 classes: '1', '2', '3', '4', '5' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 4190, 4192, 4190, 4190, 4191, 4191, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  cp    Accuracy   Kappa    \n",
       "  0.00  0.4641210  0.2867510\n",
       "  0.01  0.4935627  0.2805461\n",
       "  0.02  0.4935627  0.2805461\n",
       "  0.03  0.4935627  0.2805461\n",
       "  0.04  0.4935627  0.2805461\n",
       "  0.05  0.4935627  0.2805461\n",
       "  0.06  0.4935627  0.2805461\n",
       "  0.07  0.4935627  0.2805461\n",
       "  0.08  0.4935627  0.2805461\n",
       "  0.09  0.4935627  0.2805461\n",
       "  0.10  0.4935627  0.2805461\n",
       "\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final value used for the model was cp = 0.1."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tr_last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model for Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE will be the objective parameter for evaluation of the model. In this perpective, **cp=0.1 and minbucket = 8** is the best tuned parameter for this model for Student Evaluation Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy \n",
      "0.4914089 \n"
     ]
    }
   ],
   "source": [
    "print(dtr$overall[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the homework description, only mtry namely number of selected feature for model is modified. 4 different number is determined. Best model is achived the model with the 10 number of feature. Model results are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "4656 samples\n",
       "  32 predictor\n",
       "   5 classes: '1', '2', '3', '4', '5' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 1 times) \n",
       "Summary of sample sizes: 4190, 4191, 4190, 4190, 4190, 4192, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  Accuracy   Kappa    \n",
       "   4    0.4374783  0.2109921\n",
       "   8    0.4819396  0.2853275\n",
       "  10    0.4886044  0.2969695\n",
       "  15    0.4920430  0.3057549\n",
       "\n",
       "Tuning parameter 'splitrule' was held constant at a value of extratrees\n",
       "\n",
       "Tuning parameter 'min.node.size' was held constant at a value of 5\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were mtry = 15, splitrule = extratrees\n",
       " and min.node.size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model for Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE will be the objective parameter for evaluation of the model. In this perpective, **mtry=15** is the best tuned parameter for this model for Student Evaluation Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy \n",
      "0.507732 \n"
     ]
    }
   ],
   "source": [
    "print(rfr$overall[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different 5 shrinkage value, 3 interaction.depth, 5 n.trees values** are tried to obtain the best model. This model is the one which takes the most time to create the models. The reason behind this fact caused from number of tuned parameter in the approach. However, it has the best Accuracy value with Random Forest. Model results are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stochastic Gradient Boosting \n",
       "\n",
       "4656 samples\n",
       "  32 predictor\n",
       "   5 classes: '1', '2', '3', '4', '5' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 1 times) \n",
       "Summary of sample sizes: 4191, 4191, 4191, 4190, 4190, 4189, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  shrinkage  interaction.depth  n.trees  Accuracy   Kappa    \n",
       "  0.1        1                   50      0.5045147  0.3078019\n",
       "  0.1        1                  100      0.5116004  0.3253585\n",
       "  0.1        1                  150      0.5128862  0.3312142\n",
       "  0.1        1                  200      0.5189012  0.3425569\n",
       "  0.1        1                  250      0.5199714  0.3450251\n",
       "  0.1        3                   50      0.5116027  0.3307548\n",
       "  0.1        3                  100      0.5150413  0.3412033\n",
       "  0.1        3                  150      0.5173949  0.3463261\n",
       "  0.1        3                  200      0.5131017  0.3420885\n",
       "  0.1        3                  250      0.5154663  0.3455269\n",
       "  0.1        5                   50      0.5221169  0.3483885\n",
       "  0.1        5                  100      0.5186834  0.3484588\n",
       "  0.1        5                  150      0.5088006  0.3374475\n",
       "  0.1        5                  200      0.5036458  0.3321000\n",
       "  0.1        5                  250      0.5060132  0.3359096\n",
       "  0.3        1                   50      0.5060192  0.3232565\n",
       "  0.3        1                  100      0.5176113  0.3432444\n",
       "  0.3        1                  150      0.5139702  0.3397873\n",
       "  0.3        1                  200      0.5126789  0.3391042\n",
       "  0.3        1                  250      0.5126766  0.3395951\n",
       "  0.3        3                   50      0.5060077  0.3321892\n",
       "  0.3        3                  100      0.4991361  0.3268248\n",
       "  0.3        3                  150      0.4957054  0.3237940\n",
       "  0.3        3                  200      0.4946324  0.3238476\n",
       "  0.3        3                  250      0.4853925  0.3122594\n",
       "  0.3        5                   50      0.4963469  0.3233189\n",
       "  0.3        5                  100      0.4942009  0.3236316\n",
       "  0.3        5                  150      0.4860349  0.3132129\n",
       "  0.3        5                  200      0.4804522  0.3073159\n",
       "  0.3        5                  250      0.4768019  0.3029899\n",
       "  0.5        1                   50      0.5103133  0.3317787\n",
       "  0.5        1                  100      0.5096709  0.3342149\n",
       "  0.5        1                  150      0.5092362  0.3356185\n",
       "  0.5        1                  200      0.5038696  0.3274632\n",
       "  0.5        1                  250      0.5103120  0.3385400\n",
       "  0.5        3                   50      0.5010790  0.3317603\n",
       "  0.5        3                  100      0.4821731  0.3081454\n",
       "  0.5        3                  150      0.4768032  0.3023704\n",
       "  0.5        3                  200      0.4692708  0.2936197\n",
       "  0.5        3                  250      0.4666976  0.2907881\n",
       "  0.5        5                   50      0.4821699  0.3085435\n",
       "  0.5        5                  100      0.4750957  0.3022930\n",
       "  0.5        5                  150      0.4637080  0.2876027\n",
       "  0.5        5                  200      0.4557584  0.2760911\n",
       "  0.5        5                  250      0.4525368  0.2733735\n",
       "\n",
       "Tuning parameter 'n.minobsinnode' was held constant at a value of 20\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were n.trees = 50, interaction.depth =\n",
       " 5, shrinkage = 0.1 and n.minobsinnode = 20."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gbm_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model for Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE will be the objective parameter for evaluation of the model. In this perpective, **n.trees = 50 interaction.depth =\n",
    " 5, shrinkage = 0.1 and n.minobsinnode = 20** is the best tuned parameter for this model for Student Evaluation Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy \n",
      "0.5146048 \n"
     ]
    }
   ],
   "source": [
    "print(sgbr$overall[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparision of Results in Training and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=list(\"Linear Regression with Lasso Results\"=lm_model$results$Accuracy[which.min(lm_model$results$Accuracy)],\n",
    "            \"Decision Tree Results\"=    tr_last$results$Accuracy[which.min(tr_last$results$Accuracy)],\n",
    "            \"Random Forest Results\"=    rf_fit$results$Accuracy[which.min(rf_fit$results$Accuracy)],\n",
    "            \"Result Stochastic Gradient Boosting Results\"=    gbm_fit$results$Accuracy[which.min(gbm_fit$results$Accuracy)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy in The Train Dataset¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Linear Regression with Lasso Results</th><th scope=col>Decision Tree Results</th><th scope=col>Random Forest Results</th><th scope=col>Result Stochastic Gradient Boosting Results</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.3047682</td><td>0.4652249</td><td>0.4374783</td><td>0.4525368</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{llll}\n",
       " Linear Regression with Lasso Results & Decision Tree Results & Random Forest Results & Result Stochastic Gradient Boosting Results\\\\\n",
       "\\hline\n",
       "\t 0.3047682 & 0.4652249 & 0.4374783 & 0.4525368\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Linear Regression with Lasso Results | Decision Tree Results | Random Forest Results | Result Stochastic Gradient Boosting Results |\n",
       "|---|---|---|---|\n",
       "| 0.3047682 | 0.4652249 | 0.4374783 | 0.4525368 |\n",
       "\n"
      ],
      "text/plain": [
       "     Linear Regression with Lasso Results Decision Tree Results\n",
       "[1,] 0.3047682                            0.4652249            \n",
       "     Random Forest Results Result Stochastic Gradient Boosting Results\n",
       "[1,] 0.4374783             0.4525368                                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=list(\"Linear Regression with Lasso Results\"=lrp$overall[1],\n",
    "            \"Decision Tree Results\"=    dtr$overall[1],\n",
    "            \"Random Forest Results\"=    rfr$overall[1],\n",
    "            \"Result Stochastic Gradient Boosting Results\"=    sgbr$overall[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy in The Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Linear Regression with Lasso Results</th><th scope=col>Decision Tree Results</th><th scope=col>Random Forest Results</th><th scope=col>Result Stochastic Gradient Boosting Results</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.4750859</td><td>0.4914089</td><td>0.507732 </td><td>0.5146048</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{llll}\n",
       " Linear Regression with Lasso Results & Decision Tree Results & Random Forest Results & Result Stochastic Gradient Boosting Results\\\\\n",
       "\\hline\n",
       "\t 0.4750859 & 0.4914089 & 0.507732  & 0.5146048\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Linear Regression with Lasso Results | Decision Tree Results | Random Forest Results | Result Stochastic Gradient Boosting Results |\n",
       "|---|---|---|---|\n",
       "| 0.4750859 | 0.4914089 | 0.507732  | 0.5146048 |\n",
       "\n"
      ],
      "text/plain": [
       "     Linear Regression with Lasso Results Decision Tree Results\n",
       "[1,] 0.4750859                            0.4914089            \n",
       "     Random Forest Results Result Stochastic Gradient Boosting Results\n",
       "[1,] 0.507732              0.5146048                                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models got similar performance as achieved in the training dataset. Namely, it is hard to say that there is an over-fitting or under-fitting. Student Evaluation values can be described as nulti-classification problem, so having the worst performance measure both training and test data sets aren't a suprise for Linear Regression model. In addition to under/over-fitting, it can be said that error rates are consistent for both datasets, which can be observed looking the correlation between the results pf performance of the models in both dataset. Interestingly, all the models have more accuracy in the test dataset. However, accuracy can be bad performance measure for this type dataset(imbalance in the dataset) because by having a bias in the major part of the class, model still can get better accuracy at the end. The reason behind this fact occured from the formula of the Accuracy measurement. So, F1-Score or Recall can be more reasonable to compare the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Mail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction   0   1\n",
       "         0 538  33\n",
       "         1  20 130\n",
       "                                          \n",
       "               Accuracy : 0.9265          \n",
       "                 95% CI : (0.9049, 0.9445)\n",
       "    No Information Rate : 0.7739          \n",
       "    P-Value [Acc > NIR] : < 2e-16         \n",
       "                                          \n",
       "                  Kappa : 0.7838          \n",
       "                                          \n",
       " Mcnemar's Test P-Value : 0.09929         \n",
       "                                          \n",
       "              Precision : 0.9422          \n",
       "                 Recall : 0.9642          \n",
       "                     F1 : 0.9531          \n",
       "             Prevalence : 0.7739          \n",
       "         Detection Rate : 0.7462          \n",
       "   Detection Prevalence : 0.7920          \n",
       "      Balanced Accuracy : 0.8809          \n",
       "                                          \n",
       "       'Positive' Class : 0               \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrp=confusionMatrix(data = as.factor(prediction_pra_mae_spam_min), reference = as.factor(test_spam$X1), mode = \"prec_recall\")\n",
    "lrp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction   0   1\n",
       "         0 533  30\n",
       "         1  25 133\n",
       "                                         \n",
       "               Accuracy : 0.9237         \n",
       "                 95% CI : (0.9019, 0.942)\n",
       "    No Information Rate : 0.7739         \n",
       "    P-Value [Acc > NIR] : <2e-16         \n",
       "                                         \n",
       "                  Kappa : 0.7796         \n",
       "                                         \n",
       " Mcnemar's Test P-Value : 0.5896         \n",
       "                                         \n",
       "              Precision : 0.9467         \n",
       "                 Recall : 0.9552         \n",
       "                     F1 : 0.9509         \n",
       "             Prevalence : 0.7739         \n",
       "         Detection Rate : 0.7393         \n",
       "   Detection Prevalence : 0.7809         \n",
       "      Balanced Accuracy : 0.8856         \n",
       "                                         \n",
       "       'Positive' Class : 0              \n",
       "                                         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dtr=confusionMatrix(data = as.factor(predicted_spam), reference = as.factor(test_spam$X1), mode = \"prec_recall\")\n",
    "dtr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction   0   1\n",
       "         0 547  18\n",
       "         1  11 145\n",
       "                                          \n",
       "               Accuracy : 0.9598          \n",
       "                 95% CI : (0.9427, 0.9729)\n",
       "    No Information Rate : 0.7739          \n",
       "    P-Value [Acc > NIR] : <2e-16          \n",
       "                                          \n",
       "                  Kappa : 0.8833          \n",
       "                                          \n",
       " Mcnemar's Test P-Value : 0.2652          \n",
       "                                          \n",
       "              Precision : 0.9681          \n",
       "                 Recall : 0.9803          \n",
       "                     F1 : 0.9742          \n",
       "             Prevalence : 0.7739          \n",
       "         Detection Rate : 0.7587          \n",
       "   Detection Prevalence : 0.7836          \n",
       "      Balanced Accuracy : 0.9349          \n",
       "                                          \n",
       "       'Positive' Class : 0               \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rfr=confusionMatrix(data = as.factor(PredictRandomForest_spam), reference = as.factor(test_spam$X1), mode = \"prec_recall\")\n",
    "rfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction   0   1\n",
       "         0 545  16\n",
       "         1  13 147\n",
       "                                          \n",
       "               Accuracy : 0.9598          \n",
       "                 95% CI : (0.9427, 0.9729)\n",
       "    No Information Rate : 0.7739          \n",
       "    P-Value [Acc > NIR] : <2e-16          \n",
       "                                          \n",
       "                  Kappa : 0.8843          \n",
       "                                          \n",
       " Mcnemar's Test P-Value : 0.7103          \n",
       "                                          \n",
       "              Precision : 0.9715          \n",
       "                 Recall : 0.9767          \n",
       "                     F1 : 0.9741          \n",
       "             Prevalence : 0.7739          \n",
       "         Detection Rate : 0.7559          \n",
       "   Detection Prevalence : 0.7781          \n",
       "      Balanced Accuracy : 0.9393          \n",
       "                                          \n",
       "       'Positive' Class : 0               \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sgbr=confusionMatrix(data = as.factor(predicted_spam_sgb), reference = as.factor(test_spam$X1), mode = \"prec_recall\")\n",
    "sgbr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking the Accuracy, it can be said that best performance are optained in the Random Forest and Stochastic Gradient Boosting. Indeed, these 2 model's performance is too similar, which can be understood by looking the Precision, Recall, and F1 score of the models. The biggest difference occurs in the precision and recall values of the model. So, main objective can determine which model must be used. Moreover, performance of the other two model is also similar. Indeed decision tree's approach is more proper for classification problems. The reason behind the similarity can be explained by complexity of the models. Namely, Decision Tree's complexity is less than constructed Linear Regression with lasso, but it still has similar performance compared to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different Penalty types are tried in the glmnet package by using mean absolute percentage error and mean square error objective. In these 2 penalty, 3 different lambdas are tried to obtain a model. In addition, all the possible lambda values are checked by plotting the cv.glmnet() results. Predictions are made based on these models. Best model performance are observed in this test dataset. After all, caret() packages are used to make a model can be comparable with other method's models. Namely, caret library is used in all the methods to obtain similar report types. Penalized Regression Approach is made with 3 different lambda values. These are lambda.min, lambda.1se, and random a lambda value achieved in the previous cv.glmnet() process. Model results are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "glmnet \n",
       "\n",
       "2880 samples\n",
       "  57 predictor\n",
       "   2 classes: '0', '1' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 1 times) \n",
       "Summary of sample sizes: 2592, 2592, 2592, 2592, 2592, 2592, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  lambda        Accuracy   Kappa    \n",
       "  1.633465e-05  0.9309028  0.7962956\n",
       "  1.050003e-04  0.9309028  0.7960720\n",
       "  6.442722e-02  0.8225694  0.3270203\n",
       "\n",
       "Tuning parameter 'alpha' was held constant at a value of 1\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were alpha = 1 and lambda = 0.0001050003."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model for Penalized Regression Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE will be the objective parameter for evaluation of the model. In this perpective, **alpha = 1, lambda = 0.0001050003 and MAE Penalty type** is the best tuned parameter for this model for Spam Mail Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy \n",
      "0.926491 \n"
     ]
    }
   ],
   "source": [
    "print(lrp$overall[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6 different minbucket** values are tried to get best model for Decision Tree. However, in caret library there is no available packages for rpart for tuning the minimal number of observation per tree leaf. So for loop is tried to get information related to effects in the change of number of variable in the tree leaf. In addition, all models tested with **10 different cp value**. After looking the created model's result, the best model is selected. The best Accuracy value is selected with considering the minbucket values and cp values. Selected model evaluated 1 more time and named as tr_last. Model results are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CART \n",
       "\n",
       "2880 samples\n",
       "  57 predictor\n",
       "   2 classes: '0', '1' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 2592, 2592, 2592, 2592, 2592, 2592, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  cp    Accuracy   Kappa    \n",
       "  0.00  0.9284722  0.7906848\n",
       "  0.01  0.9246528  0.7753364\n",
       "  0.02  0.9149306  0.7450010\n",
       "  0.03  0.9086806  0.7235983\n",
       "  0.04  0.9097222  0.7250175\n",
       "  0.05  0.9097222  0.7250175\n",
       "  0.06  0.9076389  0.7198955\n",
       "  0.07  0.9076389  0.7198955\n",
       "  0.08  0.9076389  0.7198955\n",
       "  0.09  0.8979167  0.6961644\n",
       "  0.10  0.8934028  0.6851602\n",
       "\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final value used for the model was cp = 0."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tr_last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model for Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE will be the objective parameter for evaluation of the model. In this perpective, **cp=0 and minbucket = 10** is the best tuned parameter for this model for Spam Mail Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy \n",
      "0.9237171 \n"
     ]
    }
   ],
   "source": [
    "print(dtr$overall[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the homework description, only mtry namely number of selected feature for model is modified. 4 different number is determined. Best model is achived the model with the 10 number of feature. Model results are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "2880 samples\n",
       "  57 predictor\n",
       "   2 classes: '0', '1' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 1 times) \n",
       "Summary of sample sizes: 2592, 2592, 2592, 2592, 2592, 2592, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  Accuracy   Kappa    \n",
       "   4    0.9475694  0.8420506\n",
       "   8    0.9538194  0.8625748\n",
       "  10    0.9545139  0.8654099\n",
       "  15    0.9527778  0.8597787\n",
       "\n",
       "Tuning parameter 'splitrule' was held constant at a value of extratrees\n",
       "\n",
       "Tuning parameter 'min.node.size' was held constant at a value of 5\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were mtry = 10, splitrule = extratrees\n",
       " and min.node.size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model for Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE will be the objective parameter for evaluation of the model. In this perpective, **mtry=10** is the best tuned parameter for this model for Spam Mail Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy \n",
      "0.9597781 \n"
     ]
    }
   ],
   "source": [
    "print(rfr$overall[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different 5 shrinkage value, 3 interaction.depth, 5 n.trees values** are tried to obtain the best model. This model is the one which takes the most time to create the models. The reason behind this fact caused from number of tuned parameter in the approach. However, it has the best Accuracy value with Random Forest. Model results are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stochastic Gradient Boosting \n",
       "\n",
       "2880 samples\n",
       "  57 predictor\n",
       "   2 classes: '0', '1' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 1 times) \n",
       "Summary of sample sizes: 2592, 2592, 2592, 2592, 2592, 2592, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  shrinkage  interaction.depth  n.trees  Accuracy   Kappa    \n",
       "  0.1        1                   50      0.9267361  0.7729207\n",
       "  0.1        1                  100      0.9416667  0.8245584\n",
       "  0.1        1                  150      0.9447917  0.8349425\n",
       "  0.1        1                  200      0.9454861  0.8373744\n",
       "  0.1        1                  250      0.9451389  0.8367385\n",
       "  0.1        3                   50      0.9454861  0.8371056\n",
       "  0.1        3                  100      0.9500000  0.8522543\n",
       "  0.1        3                  150      0.9513889  0.8565429\n",
       "  0.1        3                  200      0.9517361  0.8579397\n",
       "  0.1        3                  250      0.9552083  0.8685325\n",
       "  0.1        5                   50      0.9493056  0.8495489\n",
       "  0.1        5                  100      0.9513889  0.8567872\n",
       "  0.1        5                  150      0.9527778  0.8609011\n",
       "  0.1        5                  200      0.9548611  0.8676870\n",
       "  0.1        5                  250      0.9538194  0.8647661\n",
       "  0.3        1                   50      0.9472222  0.8425043\n",
       "  0.3        1                  100      0.9468750  0.8420356\n",
       "  0.3        1                  150      0.9475694  0.8435273\n",
       "  0.3        1                  200      0.9496528  0.8511846\n",
       "  0.3        1                  250      0.9482639  0.8464367\n",
       "  0.3        3                   50      0.9486111  0.8482189\n",
       "  0.3        3                  100      0.9510417  0.8566427\n",
       "  0.3        3                  150      0.9503472  0.8546606\n",
       "  0.3        3                  200      0.9503472  0.8542272\n",
       "  0.3        3                  250      0.9513889  0.8578558\n",
       "  0.3        5                   50      0.9538194  0.8656637\n",
       "  0.3        5                  100      0.9545139  0.8682425\n",
       "  0.3        5                  150      0.9534722  0.8652767\n",
       "  0.3        5                  200      0.9545139  0.8679997\n",
       "  0.3        5                  250      0.9534722  0.8658951\n",
       "  0.5        1                   50      0.9451389  0.8375765\n",
       "  0.5        1                  100      0.9489583  0.8486332\n",
       "  0.5        1                  150      0.9472222  0.8434787\n",
       "  0.5        1                  200      0.9461806  0.8407719\n",
       "  0.5        1                  250      0.9489583  0.8503366\n",
       "  0.5        3                   50      0.9482639  0.8473022\n",
       "  0.5        3                  100      0.9496528  0.8539205\n",
       "  0.5        3                  150      0.9510417  0.8576333\n",
       "  0.5        3                  200      0.9534722  0.8642496\n",
       "  0.5        3                  250      0.9500000  0.8545526\n",
       "  0.5        5                   50      0.9527778  0.8613930\n",
       "  0.5        5                  100      0.9541667  0.8659682\n",
       "  0.5        5                  150      0.9496528  0.8520059\n",
       "  0.5        5                  200      0.9461806  0.8435420\n",
       "  0.5        5                  250      0.9409722  0.8310891\n",
       "\n",
       "Tuning parameter 'n.minobsinnode' was held constant at a value of 20\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were n.trees = 250, interaction.depth =\n",
       " 3, shrinkage = 0.1 and n.minobsinnode = 20."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gbm_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model for Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE will be the objective parameter for evaluation of the model. In this perpective, **n.trees = 250 interaction.depth =\n",
    " 3, shrinkage = 0.1 and n.minobsinnode = 20** is the best tuned parameter for this model for Spam Mail Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy \n",
      "0.9597781 \n"
     ]
    }
   ],
   "source": [
    "print(sgbr$overall[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparision of Results in Training and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=list(\"Linear Regression with Lasso Results\"=lm_model$results$Accuracy[which.min(lm_model$results$Accuracy)],\n",
    "            \"Decision Tree Results\"=    tr_last$results$Accuracy[which.min(tr_last$results$Accuracy)],\n",
    "            \"Random Forest Results\"=    rf_fit$results$Accuracy[which.min(rf_fit$results$Accuracy)],\n",
    "            \"Result Stochastic Gradient Boosting Results\"=    gbm_fit$results$Accuracy[which.min(gbm_fit$results$Accuracy)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy in The Train Dataset¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Linear Regression with Lasso Results</th><th scope=col>Decision Tree Results</th><th scope=col>Random Forest Results</th><th scope=col>Result Stochastic Gradient Boosting Results</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.8225694</td><td>0.8934028</td><td>0.9475694</td><td>0.9267361</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{llll}\n",
       " Linear Regression with Lasso Results & Decision Tree Results & Random Forest Results & Result Stochastic Gradient Boosting Results\\\\\n",
       "\\hline\n",
       "\t 0.8225694 & 0.8934028 & 0.9475694 & 0.9267361\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Linear Regression with Lasso Results | Decision Tree Results | Random Forest Results | Result Stochastic Gradient Boosting Results |\n",
       "|---|---|---|---|\n",
       "| 0.8225694 | 0.8934028 | 0.9475694 | 0.9267361 |\n",
       "\n"
      ],
      "text/plain": [
       "     Linear Regression with Lasso Results Decision Tree Results\n",
       "[1,] 0.8225694                            0.8934028            \n",
       "     Random Forest Results Result Stochastic Gradient Boosting Results\n",
       "[1,] 0.9475694             0.9267361                                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=list(\"Linear Regression with Lasso Results\"=lrp$overall[1],\n",
    "            \"Decision Tree Results\"=    dtr$overall[1],\n",
    "            \"Random Forest Results\"=    rfr$overall[1],\n",
    "            \"Result Stochastic Gradient Boosting Results\"=    sgbr$overall[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy in The Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Linear Regression with Lasso Results</th><th scope=col>Decision Tree Results</th><th scope=col>Random Forest Results</th><th scope=col>Result Stochastic Gradient Boosting Results</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.926491 </td><td>0.9237171</td><td>0.9597781</td><td>0.9597781</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{llll}\n",
       " Linear Regression with Lasso Results & Decision Tree Results & Random Forest Results & Result Stochastic Gradient Boosting Results\\\\\n",
       "\\hline\n",
       "\t 0.926491  & 0.9237171 & 0.9597781 & 0.9597781\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Linear Regression with Lasso Results | Decision Tree Results | Random Forest Results | Result Stochastic Gradient Boosting Results |\n",
       "|---|---|---|---|\n",
       "| 0.926491  | 0.9237171 | 0.9597781 | 0.9597781 |\n",
       "\n"
      ],
      "text/plain": [
       "     Linear Regression with Lasso Results Decision Tree Results\n",
       "[1,] 0.926491                             0.9237171            \n",
       "     Random Forest Results Result Stochastic Gradient Boosting Results\n",
       "[1,] 0.9597781             0.9597781                                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models got similar performance as achieved in the training dataset. Namely, it is hard to say that there is an over-fitting or under-fitting. Spam Estimation can be described as classification problem, so having the worst performance measure both training and test data sets aren't a suprise for Linear Regression model. In addition to under/over-fitting, it can be said that error rates are consistent for both datasets, which can be observed looking the correlation between the results pf performance of the models in both dataset. However, there is still some problems related to consistency. It can be understood by looking the how much performance measure are changed between the datasets. Interestingly, all the models have more accuracy in the test dataset. However, accuracy can be bad performance measure for this type dataset(imbalance in the dataset) because by having a bias in the major part of the class, model still can get better accuracy at the end. The reason behind this fact occured from the formula of the Accuracy measurement. So, F1-Score or Recall can be more reasonable to compare the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be said that trees' performance is better than linear regression in classification problems, which can be observed for both binary and multi classification. In addition, Random Forest and Stochastic Gradient Boosting' performance are better than normal decision tree. This fact exist not only in classification problem but also in regression problems. Morever, the worst performance observed in the multinomial classification. \n",
    "\n",
    "For the Regression problems, the worst performance belongs to Decision Tree. The reason behind this fact can be explained by the nature of the algorithms. Namely, extrapolation doesn't exist in the Decision Tree, which makes it performance to worse. In addition, Random Forest and Stochastic Gradient Boosting models' performance is better than the other tree model. And in some cases, these models have better performance compared to Linear Regression with Lasso. Indeed, this situation can be occured because of different reasons. However, the major difference can be complexity difference between the model. In other words, Random Forest and Stochastic Gradient Boosting have more complex stucture(not over-fitting). This situation appears by looking the how much times passed in the creation of the model. To sum up, Linear Regression model's nature is more proper for regression problems and the better model can be obtained in the limited time period by Linear Regression. \n",
    "\n",
    "In addition to performance measure, Stochastic Gradient Boosting has more flexibility to tune its parameters. This fact can cause extra effort to obtain the optimal parameter combination. However, tuning is a good option for getting the best model at the end. In addition, computers' CPU and GPU performance is more advanced and cloud systems exist in the world, which makes this model to the better among the other models. In addition, I have a project related to this XGBOOST library. In this project, I got a lot of evidence related to having good performance of XGBOOST. This fact is approved one more time in this homework.\n",
    "\n",
    "Lastly, there is no problem related to overfitting. For some models(not too much complex) can have some underfitting problems in the training dataset. But still there is no solid evidence for this assumption. In addition, train error and test error of models have correlation. So, it can be said that all train and test datasets are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
